<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Burn Diagnosis AI Challenge | June Lee</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    <style>
        .project-detail-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 48px 20px;
        }
        
        .project-detail-header {
            margin-bottom: 48px;
            text-align: center;
        }
        
        .project-detail-header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #333;
            margin-bottom: 16px;
        }
        
        .project-detail-meta {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 24px;
        }
        
        .project-detail-image {
            width: 100%;
            max-width: 600px;
            height: 300px;
            object-fit: contain;
            background: white;
            border-radius: 12px;
            margin: 0 auto 32px auto;
            display: block;
        }
        
        .project-detail-content {
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            border: 1px solid #e5e7eb;
            padding: 2rem;
            margin-bottom: 32px;
        }
        
        .project-detail-content h2 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #333;
            margin-bottom: 1rem;
            margin-top: 2rem;
        }
        
        .project-detail-content h2:first-child {
            margin-top: 0;
        }
        
        .project-detail-content h3 {
            font-size: 1.2rem;
            font-weight: 600;
            color: #333;
            margin-bottom: 0.75rem;
            margin-top: 1.5rem;
        }
        
        .project-detail-content p {
            color: #555;
            line-height: 1.7;
            margin-bottom: 1rem;
        }
        
        .project-detail-content ul, .project-detail-content ol {
            margin-bottom: 1rem;
            padding-left: 1.5rem;
        }
        
        .project-detail-content li {
            color: #555;
            line-height: 1.6;
            margin-bottom: 0.5rem;
        }
        
        .project-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1.5rem;
        }
        
        .project-tags .tag {
            background: #f0f9ff;
            color: #0369a1;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
            border: 1px solid #e0f2fe;
        }
        
        .back-button {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 12px 24px;
            background: #333;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s ease;
            margin-bottom: 32px;
        }
        
        .back-button:hover {
            background: #000;
            transform: translateY(-1px);
        }
        
        .two-col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 1.5rem 0;
        }
        
        .timeline {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .timeline .item {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 1rem;
            border-left: 4px solid #0369a1;
        }
        
        .achievement-badge {
            background: linear-gradient(135deg, #fbbf24, #f59e0b);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.9rem;
            font-weight: 600;
            display: inline-block;
            margin-bottom: 1rem;
        }
        
        @media (max-width: 768px) {
            .two-col {
                grid-template-columns: 1fr;
            }
            
            .project-detail-header h1 {
                font-size: 2rem;
            }
        }
    </style>
</head>
<body>
    <div class="project-detail-container">
        <a href="../index.html" class="back-button">‚Üê Back to Portfolio</a>
        
        <div class="project-detail-header">
            <h1>üèÜ Burn Diagnosis AI Challenge</h1>
            <div class="project-detail-meta">2022 ‚Äî AI-Powered Burn Severity Classification</div>
            <div class="project-detail-meta">Seoul National University Hospital ‚Ä¢ Ministry of Science and ICT ‚Ä¢ National Information Society Agency</div>
        </div>

        <img src="../pics/skin_burn.png" alt="Burn Diagnosis AI Challenge" class="project-detail-image">
        
        <div style="text-align: center; margin: 32px 0;">
            <img src="../pics/burn-diagnosis-3rd-place.pdf" alt="3rd Place Certificate" style="max-width: 600px; width: 100%; height: auto; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
            <p style="margin-top: 16px; color: #666; font-style: italic;">3rd Place Award Certificate - Seoul National University Hospital AI Challenge</p>
        </div>

        <div class="project-detail-content">
            <div class="achievement-badge">üèÖ 3rd Place Winner</div>
            
            <div class="project-tags">
                <span class="tag">Computer Vision</span>
                <span class="tag">Medical AI</span>
                <span class="tag">Emergency Medicine</span>
            </div>

            <h2>Project Overview</h2>
            <p>Created an intelligent burn classification system to help emergency medical professionals make faster, more accurate treatment decisions when every second counts. This project won 3rd place in the national AI challenge hosted by Seoul National University Hospital.</p>

            <h2>Detailed Introduction</h2>
            <p><strong>Challenge context.</strong> Burn injuries are among the most devastating traumatic injuries, requiring immediate and accurate assessment for optimal patient outcomes. Traditional burn assessment relies heavily on clinical experience and visual inspection, which can be subjective and time-consuming. The challenge aimed to develop AI systems that could assist healthcare professionals in rapidly and accurately classifying burn severity from medical images.</p>

            <h3>Key project objectives</h3>
            <ul>
              <li>Develop a computer vision model to detect region-of-interest for burn and classify the severity of the burn.</li>
              <li>Create a robust model that works across diverse patient populations and burn types.</li>
              <li>Ensure high accuracy in critical emergency medicine scenarios.</li>
              <li>Provide interpretable results to support clinical decision-making.</li>
              <li>Optimize for real-time performance in emergency settings.</li>
            </ul>

            <h3>Background: Burn Classification in Emergency Medicine</h3>
            <p>Burn injuries are classified by depth and extent, with different treatment protocols for each category. First-degree burns affect only the epidermis, second-degree burns extend into the dermis, and third-degree burns involve deeper tissues. Accurate classification is crucial for determining treatment urgency, pain management, and potential need for specialized care.</p>

            <h3>Technical approach</h3>
            <p>Our solution implemented a dual-model architecture combining object detection and classification:</p>
            <ul>
              <li><strong>Data preprocessing:</strong> Image resizing to 256x256, normalization, and tensor conversion using PyTorch transforms.</li>
              <li><strong>Regression model (Bounding Box Detection):</strong> ResNet34-based architecture with custom FC layer (512‚Üí4) for burn region localization using IoU loss.</li>
              <li><strong>Classification model (Severity Classification):</strong> ResNet34 architecture with 5-class output for burn severity classification (0-4 stages).</li>
              <li><strong>Training strategy:</strong> Adam optimizer with learning rate scheduling (5e-3 ‚Üí 3.2e-3 ‚Üí 2.56e-3 ‚Üí 2.048e-3) and progressive training over 6 iterations.</li>
              <li><strong>Loss functions:</strong> HuberLoss for regression (delta=1.0) and CrossEntropyLoss for classification.</li>
              <li><strong>Evaluation metrics:</strong> IoU (Intersection over Union) for bounding box accuracy and classification accuracy for severity prediction.</li>
            </ul>

            <h3>Dataset & validation</h3>
            <ol>
              <li><strong>Dataset structure:</strong> Organized into Training/Validation/Test splits with JPG images and corresponding JSON annotation files containing bounding box coordinates and severity labels.</li>
              <li><strong>Data format:</strong> Custom BurnDataset class handling image loading, JSON parsing for bbox coordinates (x, y, w, h) normalized to [0,1] range, and 5-class severity labels (0-4).</li>
              <li><strong>Data loading:</strong> DataLoader with batch_size=32 for training, batch_size=1 for validation/testing, 24 workers for parallel processing, and pin_memory for GPU optimization.</li>
              <li><strong>Validation strategy:</strong> Real-time validation every 32 steps during training with IoU calculation for regression and accuracy metrics for classification.</li>
              <li><strong>Test evaluation:</strong> Comprehensive testing with inference time measurement, per-class performance analysis, and visualization of bounding box predictions.</li>
            </ol>

            <h3>Model development strategy</h3>
            <p>Our development process implemented a systematic dual-model approach:</p>
            <ol>
              <li><strong>Regression model development:</strong> ResNet34 backbone with custom FC layer for bounding box regression, trained using HuberLoss with progressive learning rate decay over 6 training cycles (512 iterations each).</li>
              <li><strong>Classification model development:</strong> Separate ResNet34 architecture for 5-class severity classification, trained with CrossEntropyLoss and StepLR scheduler (gamma=0.75, step_size=5).</li>
              <li><strong>Training optimization:</strong> Progressive learning rate reduction (5e-3 ‚Üí 4e-3 ‚Üí 3.2e-3 ‚Üí 2.56e-3 ‚Üí 2.048e-3) with model checkpointing based on validation IoU scores.</li>
              <li><strong>Evaluation framework:</strong> Real-time validation every 32 steps with IoU calculation for regression accuracy and classification accuracy metrics, with comprehensive test evaluation including inference time analysis.</li>
              <li><strong>Model integration:</strong> Combined inference pipeline with regression model for burn region detection followed by classification model for severity assessment.</li>
            </ol>

            <h3>Performance & results</h3>
            <p>Our system achieved outstanding performance across all burn severity classes:</p>
            <ul>
              <li><strong>Overall performance:</strong> mAP@IoU(0.5) of 0.89, IoU of 0.76, with 0.76 precision and 0.76 recall.</li>
              <li><strong>Class-specific performance:</strong>
                <ul>
                  <li>1st Class: mAP@IoU(0.5) 0.86, IoU 0.68</li>
                  <li>2nd Class: mAP@IoU(0.5) 0.95, IoU 0.73, Perfect precision/recall (1.0)</li>
                  <li>2nd-Deep Class: mAP@IoU(0.5) 0.96, IoU 0.80</li>
                  <li>3rd Class: mAP@IoU(0.5) 0.98, IoU 0.82, Perfect precision/recall (1.0)</li>
                  <li>4th Class: mAP@IoU(0.5) 0.71, IoU 0.80</li>
                </ul>
              </li>
              <li><strong>Inference speed:</strong> Ultra-fast processing with average inference time of 0.0188 seconds per image.</li>
              <li><strong>Real-time capability:</strong> Consistent sub-20ms inference across all severity classes, enabling real-time clinical decision support.</li>
            </ul>

            <h3>Team & roles</h3>
            <div class="two-col">
              <div>
                <p><strong>Core team</strong></p>
                <ul>
                  <li>BS June Lee ‚Äî Project Lead, Computer Vision & Model Development</li>
                  <li>BS Woo-Jin Jeong ‚Äî Data Engineering & Preprocessing</li>
                  <li>PhD Jeong-Hwa Kang ‚Äî Validation & Evaluation</li>
                </ul>
              </div>
              <div>
                <p><strong>Clinical partners</strong></p>
                <ul>
                  <li>Department of Plastic and Reconstructive Surgery, SNUH</li>
                  <li>Emergency Medicine specialists</li>
                  <li>Clinical informatics team</li>
                  <li>Medical imaging experts</li>
                </ul>
              </div>
            </div>

            <h3>Technical implementation</h3>
            <ul>
              <li><strong>Image processing:</strong> Advanced preprocessing pipeline for medical image enhancement.</li>
              <li><strong>Model architecture:</strong> Custom CNN with attention mechanisms for burn region focus.</li>
              <li><strong>Training strategy:</strong> Multi-stage training with progressive complexity increase.</li>
              <li><strong>Inference optimization:</strong> Model quantization and optimization for real-time deployment.</li>
            </ul>

            <h3>Clinical impact</h3>
            <div class="timeline">
              <div class="item"><strong>Emergency triage:</strong> Rapid burn assessment in emergency departments.</div>
              <div class="item"><strong>Telemedicine:</strong> Remote burn assessment for rural and underserved areas.</div>
              <div class="item"><strong>Training support:</strong> Educational tool for medical students and residents.</div>
              <div class="item"><strong>Quality assurance:</strong> Second opinion system for burn classification.</div>
            </div>

            <h3>Challenges & solutions</h3>
            <ul>
              <li><strong>Image variability:</strong> Robust preprocessing and data augmentation techniques.</li>
              <li><strong>Clinical accuracy:</strong> Extensive validation with expert clinicians.</li>
              <li><strong>Real-time performance:</strong> Model optimization and efficient inference pipelines.</li>
              <li><strong>Generalization:</strong> Diverse training data and cross-institutional validation.</li>
            </ul>

            <h3>Future directions</h3>
            <ul>
              <li>Integration with electronic health records (EHR) systems.</li>
              <li>Mobile application for point-of-care burn assessment.</li>
              <li>Expansion to other types of traumatic injuries.</li>
              <li>Multi-modal analysis combining images with patient history.</li>
            </ul>

            <h3>Recognition & impact</h3>
            <p>This project received 3rd place in the national AI challenge, demonstrating the potential of AI in emergency medicine. The work has been presented at medical AI conferences and has contributed to ongoing research in computer-aided diagnosis for traumatic injuries.</p>

        </div>
    </div>
</body>
</html>
