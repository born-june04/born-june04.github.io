<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SpeAKN: ALS Communication System | June Lee</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    <style>
        .project-detail-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 48px 20px;
        }
        
        .project-detail-header {
            margin-bottom: 48px;
            text-align: center;
        }
        
        .project-detail-header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #333;
            margin-bottom: 16px;
        }
        
        .project-detail-meta {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 24px;
        }
        
        .project-detail-image {
            width: 100%;
            max-width: 600px;
            height: 300px;
            object-fit: contain;
            background: white;
            border-radius: 12px;
            margin: 0 auto 32px auto;
            display: block;
        }
        
        .figure-container {
            text-align: center;
            margin: 24px 0;
        }
        
        .figure-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        .figure-caption {
            margin-top: 8px;
            color: #666;
            font-style: italic;
            font-size: 0.9rem;
        }
        
        .project-detail-content {
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            border: 1px solid #e5e7eb;
            padding: 2rem;
            margin-bottom: 32px;
        }
        
        .project-detail-content h2 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #333;
            margin-bottom: 1rem;
            margin-top: 2rem;
        }
        
        .project-detail-content h2:first-child {
            margin-top: 0;
        }
        
        .project-detail-content h3 {
            font-size: 1.2rem;
            font-weight: 600;
            color: #333;
            margin-bottom: 0.75rem;
            margin-top: 1.5rem;
        }
        
        .project-detail-content p {
            color: #555;
            line-height: 1.7;
            margin-bottom: 1rem;
        }
        
        .project-detail-content ul, .project-detail-content ol {
            margin-bottom: 1rem;
            padding-left: 1.5rem;
        }
        
        .project-detail-content li {
            color: #555;
            line-height: 1.6;
            margin-bottom: 0.5rem;
        }
        
        .project-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-bottom: 1.5rem;
        }
        
        .project-tags .tag {
            background: #f0f9ff;
            color: #0369a1;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
            border: 1px solid #e0f2fe;
        }
        
        .back-button {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 12px 24px;
            background: #333;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            transition: all 0.3s ease;
            margin-bottom: 32px;
        }
        
        .back-button:hover {
            background: #000;
            transform: translateY(-1px);
        }
        
        .two-col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 1.5rem 0;
        }
        
        .timeline {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .timeline .item {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 1rem;
            border-left: 4px solid #0369a1;
        }
        
        .impact-badge {
            background: linear-gradient(135deg, #ec4899, #be185d);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.9rem;
            font-weight: 600;
            display: inline-block;
            margin-bottom: 1rem;
        }
        
        .performance-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        
        .performance-table th,
        .performance-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        
        .performance-table th {
            background-color: #f2f2f2;
            font-weight: 600;
        }
        
        .code-block {
            background: #f8f9fa;
            border-radius: 6px;
            padding: 1rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            margin: 1rem 0;
            overflow-x: auto;
        }
        
        @media (max-width: 768px) {
            .two-col {
                grid-template-columns: 1fr;
            }
            
            .project-detail-header h1 {
                font-size: 2rem;
            }
        }
    </style>
</head>
<body>
    <div class="project-detail-container">
        <a href="../index.html" class="back-button">← Back to Portfolio</a>
        
        <div class="project-detail-header">
            <h1>SpeAKN: Speak for ALS with Korean NLP</h1>
            <div class="project-detail-meta">2023 — AI-Powered Communication System for ALS Patients</div>
            <div class="project-detail-meta">Hankuk University of Foreign Studies • HUFS AI Project</div>
        </div>

        <div class="figure-container">
            <img src="/pics/als/fig1.png" alt="Look to Speak Application Interface" style="max-width: 400px; width: 100%; height: auto; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
            <div class="figure-caption">Figure 1: 'Look-to-Speak' Application Interface</div>
        </div>
        
        <div class="project-detail-content">
            <div class="impact-badge">Assistive Technology Research</div>
            
            <div class="project-tags">
                <span class="tag">Korean NLP</span>
                <span class="tag">Eye-Tracking</span>
                <span class="tag">Speech Recognition</span>
                <span class="tag">Transformer Models</span>
                <span class="tag">GRU Networks</span>
                <span class="tag">Assistive Technology</span>
            </div>

            <h2>Project Overview</h2>
            <p>SpeAKN (Speak for ALS with Korean NLP) is an innovative AI-powered communication system designed specifically for ALS patients who have lost their ability to speak. The system combines eye-tracking technology with advanced Korean natural language processing to provide contextually appropriate response suggestions, enabling meaningful communication for patients with progressive motor function decline.</p>

            <div class="figure-container">
                <img src="/pics/als/fig2.png" alt="AI-TRACKING System Pipeline">
                <div class="figure-caption">Figure 2: Overall model architecture overview. Two AI models were used: the first model processes voice data, and the second model uses the sentence output from the first model as input to generate final response sentences.</div>
            </div>

            <h2>Problem Statement</h2>
            <p>ALS (Amyotrophic Lateral Sclerosis) is a neurodegenerative disease in which motor nerve cells in the brain and spinal cord progressively deteriorate. Over time, ALS patients lose their ability to communicate using natural language, with 80-95% of patients requiring alternative communication methods known as AAC (Augmentative and Alternative Communication).</p>
            
            <p>Existing solutions like "Look to Speak" require users to make numerous selections before reaching their intended response, and users cannot pre-input their desired responses. Our AI-TRACKING system addresses these limitations by combining eye-tracking technology with artificial intelligence to provide contextually relevant response options.</p>

            <h2>Technical Architecture</h2>
            
            <h3>System Pipeline</h3>
            <p>The SpeAKN system operates through a two-stage AI pipeline:</p>
            <ol>
                <li><strong>Speech-to-Text (Wav2Text):</strong> Converts incoming voice questions to text using advanced speech recognition</li>
                <li><strong>Context-Aware Response Generation (Text2Text):</strong> Analyzes the textual context and generates appropriate response options</li>
                <li><strong>Eye-Tracking Selection:</strong> Patients select their intended response using eye movements</li>
            </ol>

            <div class="figure-container">
                <img src="/pics/als/fig6.png" alt="SpeAKN Model Architecture">
                <div class="figure-caption">Figure 6: Overall architecture of the SpeAKN model. Composed of Wav2Text and Text2Text models, appropriately utilizing transformer encoder and self-attention layers. Unlike existing models, a GRU layer was added to focus on learning Korean word order.</div>
            </div>

            <h2>Data and Training</h2>
            
            <h3>Dataset</h3>
            <p>This project utilized the National Institute of Korean Language's datasets:</p>
            <ul>
                <li><strong>Everyday Conversation Speech Corpus 2020:</strong> 870,162 audio files and 2,231 corresponding sentence data points</li>
                <li><strong>Everyday Conversation Corpus 2020:</strong> 2,232 sentence data points</li>
                <li><strong>Total data:</strong> ~500 hours of conversation data from 2,739 participants</li>
                <li><strong>Audio format:</strong> 16kHz PCM format</li>
                <li><strong>Text format:</strong> UTF-8 JSON encoding</li>
            </ul>

            <h3>Model Optimization</h3>
            <p>We experimentally tested various optimization approaches:</p>

            <div class="figure-container">
                <img src="/pics/als/fig3.png" alt="Optimizer Comparison">
                <div class="figure-caption">Figure 3: X-axis represents training epochs, Y-axis represents training time. Shows that the AdamW Optimizer's training time stabilizes during the learning process.</div>
            </div>

            <p>The AdamW Optimizer demonstrated superior performance with faster learning progression and better stabilization during training compared to the Sophia Optimizer.</p>

            <h3>Architecture Innovations</h3>
            <p>Our SpeAKN model employs several key innovations:</p>
            <ul>
                <li><strong>GRU Layers:</strong> Successfully overcome vanishing gradient problems, particularly effective for Korean language processing</li>
                <li><strong>Transformer Encoders:</strong> For robust feature extraction from speech and text</li>
                <li><strong>Self-Attention Mechanisms:</strong> To capture long-range dependencies in Korean conversations</li>
            </ul>

            <div class="figure-container">
                <img src="/pics/als/fig4.png" alt="LSTM vs GRU Comparison">
                <div class="figure-caption">Figure 4: (Top) LSTM layer, (Bottom) GRU layer. Moving rightward shows the parameters of learned hidden states. Compared to the LSTM layer, the GRU layer shows that red parameters (vanishing gradients) disappear at H5.</div>
            </div>

            <h3>Audio Processing Optimization</h3>
            <p>To handle varying audio lengths efficiently, we analyzed the distribution of audio data lengths:</p>

            <div class="figure-container">
                <img src="/pics/als/fig5.png" alt="Audio Length Distribution">
                <div class="figure-caption">Figure 5: Histogram representation of audio data lengths. The maximum length is 220,000 while the average is 25,000, approximately 10 times smaller. By setting audio data length to 100,000, we can avoid the curse of dimensionality.</div>
            </div>

            <p>We set the standardized audio length to 100,000 samples to balance computational efficiency with data preservation, avoiding the curse of dimensionality while maintaining essential information.</p>

            <h2>Results and Performance</h2>
            
            <h3>Initial Performance Metrics</h3>
            <table class="performance-table">
                <thead>
                    <tr>
                        <th>Model Component</th>
                        <th>Train MSE</th>
                        <th>Validation MSE</th>
                        <th>Input Type</th>
                        <th>Output Quality</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Speech2Text</td>
                        <td>10.039</td>
                        <td>12.569</td>
                        <td>Voice/Question</td>
                        <td>Moderate accuracy</td>
                    </tr>
                    <tr>
                        <td>Text2Text</td>
                        <td>11.234</td>
                        <td>13.788</td>
                        <td>Text/Answer</td>
                        <td>Requires improvement</td>
                    </tr>
                </tbody>
            </table>

            <h3>Attention Visualization</h3>
            <p>To verify model learning effectiveness, we visualized attention patterns:</p>

            <div class="figure-container">
                <img src="/pics/als/fig7.png" alt="Attention Visualization">
                <div class="figure-caption">Figure 7: Visualizing Attention - Analysis showed the model needed improvement in focusing on important linguistic features.</div>
            </div>

            <h3>Data Quality Analysis</h3>
            <p>Our analysis revealed important insights about the training data:</p>

            <div class="figure-container">
                <img src="/pics/als/fig8.png" alt="Training Data Distribution">
                <div class="figure-caption">Figure 8: Pie chart showing the data actually used for training - Only 14.3% of the original dataset consisted of meaningful question-answer pairs.</div>
            </div>

            <h3>Korean Language Challenges</h3>
            <p>We identified specific challenges related to Korean language processing:</p>

            <div class="figure-container">
                <img src="/pics/als/fig9.png" alt="Token Frequency Analysis">
                <div class="figure-caption">Figure 9: Bar chart showing tokens appearing more than 5,000 times. Korean particles such as '이' and '는' appear frequently, presenting unique processing challenges.</div>
            </div>

            <div class="figure-container">
                <img src="/pics/als/fig10.png" alt="Token Distribution">
                <div class="figure-caption">Figure 10: Pie chart showing the proportion of tokens by frequency of appearance - 53.8% of tokens appeared only once, suggesting the need for better handling of rare tokens.</div>
            </div>

            <h2>Eye-Tracking Implementation</h2>
            <p>The final system integrates eye-tracking technology for user interaction. When a question like "아픈 곳은 없어요?" (Do you have any pain?) is processed, the system generates contextually appropriate response options that patients can select through eye movements.</p>

            <div class="figure-container">
                <img src="../pics/als/Demonstration.gif" alt="ALS Communication System Demonstration" class="project-detail-image">
                <div class="figure-caption">ALS Communication System - Real-time demonstration of eye-tracking interface and communication workflow</div>
            </div>

            <h2>Technical Implementation Details</h2>
            
            <h3>Key Features</h3>
            <ul>
                <li><strong>Real-time Speech Processing:</strong> Immediate conversion of caregiver questions to text</li>
                <li><strong>Context-Aware NLP:</strong> Korean language-specific processing for accurate response generation</li>
                <li><strong>Adaptive Interface:</strong> Eye-tracking based selection system requiring minimal motor function</li>
                <li><strong>Personalization:</strong> Learning from user interaction patterns over time</li>
            </ul>

            <h3>Technical Stack</h3>
            <ul>
                <li><strong>Speech Recognition:</strong> Google SpeechRecognition API</li>
                <li><strong>Language Model:</strong> Kakao KoGPT for Korean text generation</li>
                <li><strong>Deep Learning:</strong> Custom transformer and GRU architectures</li>
                <li><strong>Eye-Tracking:</strong> Computer vision-based gaze detection</li>
            </ul>

            <h3>Team Collaboration</h3>
            <div class="two-col">
                <div>
                    <p><strong>Development Team</strong></p>
                    <ul>
                        <li>June Lee — Project Lead, NLP Architecture</li>
                        <li>Ohjoon Kwon — Speech Processing</li>
                        <li>Youngjin Jeong — Model Optimization</li>
                    </ul>
                </div>
                <div>
                    <p><strong>Research Team</strong></p>
                    <ul>
                        <li>Chaeyeon Kim — Data Analysis</li>
                        <li>Jeongmin Lee — Eye-Tracking Implementation</li>
                        <li>Faculty Advisors — Clinical Guidance</li>
                    </ul>
                </div>
            </div>

            <h2>Impact and Future Directions</h2>
            
            <h3>Clinical Significance</h3>
            <p>SpeAKN represents a significant advancement in assistive technology for ALS patients by:</p>
            <ul>
                <li>Reducing communication time and cognitive load</li>
                <li>Providing contextually relevant response options</li>
                <li>Adapting to progressive motor function decline</li>
                <li>Supporting Korean language-specific communication patterns</li>
            </ul>

            <h3>Future Enhancements</h3>
            <ul>
                <li>Integration with additional AAC modalities</li>
                <li>Advanced personalization through continuous learning</li>
                <li>Expansion to other neurodegenerative conditions</li>
                <li>Real-time emotion recognition and response adaptation</li>
                <li>Multi-language support for diverse patient populations</li>
            </ul>

            <h3>Open Source Contribution</h3>
            <p>The eye-tracking implementation is available as an open-source project: <a href="https://github.com/junhyk-lee/Look_to_Speak" target="_blank">https://github.com/junhyk-lee/Look_to_Speak</a></p>

            <h3>Research Publications</h3>
            <p>This work contributes to the growing body of research in assistive technology and demonstrates the potential of AI-powered solutions for improving quality of life for patients with neurodegenerative diseases.</p>
        </div>
    </div>
</body>
</html>